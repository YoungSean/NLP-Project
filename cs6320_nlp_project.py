# -*- coding: utf-8 -*-
"""CS6320 NLP Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DaxqNFbLa6Pf6vUD_dirAZTm__NuECd7
"""

# from google.colab import drive
# drive.mount('/content/gdrive')
#
# !pwd

"""## Yangxiao Lu
## NLP Project

## 1. Preprocess
"""

import tensorflow as tf

import pandas as pd
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
max_len = 100

relation2label = {'Other': 0,
                  "Cause-Effect(e1,e2)": 1,
                  "Cause-Effect(e2,e1)": 2,
                  "Component-Whole(e1,e2)": 3,
                  "Component-Whole(e2,e1)": 4,
                  "Entity-Destination(e1,e2)": 5,
                  "Entity-Destination(e2,e1)": 6,
                  "Entity-Origin(e1,e2)": 7,
                  "Entity-Origin(e2,e1)": 8
                  }

label2relation = {
    0: 'Other',
    1: "Cause-Effect(e1,e2)",
    2: "Cause-Effect(e2,e1)",
    3: "Component-Whole(e1,e2)",
    4: "Component-Whole(e2,e1)",
    5: "Entity-Destination(e1,e2)",
    6: "Entity-Destination(e2,e1)",
    7: "Entity-Origin(e1,e2)",
    8: "Entity-Origin(e2,e1)"
}


class HandleDataset:
    def __init__(self, file_path):
        self.file_path = file_path
        self.df =None

        self.file2dataframe()

    def file2dataframe(self):
        with open(self.file_path, "r") as file:
            lines = [line.rstrip() for line in file]
            num_lines = len(lines)
            data = []
            for i in range(0, num_lines, 4):
                id_sentence = lines[i].split("\t")
                # sentence id
                sen_id = id_sentence[0]
                # get sentence
                sent = id_sentence[1][1:-1]
                # get the relation and replace some relation with 'other'
                relation = lines[i+1]
                if relation not in relation2label:
                    relation = 'Other'
                label = relation2label[relation]
                # get positions
                pos_e1 = self.get_entity1_position(sent)
                pos_e2 = self.get_entity2_position(sent)
                # remove <e1> </e1> <e2> </e2>
                sentence = sent.replace("<e1>", "").replace("</e1>", "").replace("<e2>", "").replace("</e2>", "")
                
                # relative positions
                relative_pos1 = [i-pos_e1 for i,v in enumerate(sentence)]
                relative_pos2 = [i-pos_e2 for i,v in enumerate(sentence)]
                data.append([sen_id, sentence, relation, label, pos_e1, pos_e2, relative_pos1, relative_pos2])

            self.df = pd.DataFrame(data, columns=["sentence id", "sentences", "relation", "labels", "Positon of e1", "Positon of e2", "Relative Positon of e1", "Relative Positon of e2"])

    def get_tokens(self):
        n_most_common_words = 20000 # vocabulary size
        max_len = 100
        # Initialization
        tokenizer = Tokenizer(num_words=n_most_common_words, filters='!"#$%&()*+,-./:;=?@[]^_`{|}~', lower=True)
        # Fit and transformation
        tokenizer.fit_on_texts(self.df['sentences'].values)
        sequences = tokenizer.texts_to_sequences(self.df['sentences'].values)
        word_index = tokenizer.word_index
        print('Found %s unique tokens.' % len(word_index))
        # Padding
        X = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len)
        X_pos1 = tf.keras.preprocessing.sequence.pad_sequences(self.df['Relative Positon of e1'].values, maxlen=max_len)
        X_pos2 = tf.keras.preprocessing.sequence.pad_sequences(self.df['Relative Positon of e2'].values, maxlen=max_len)
        labels = tf.keras.utils.to_categorical(self.df['labels'], num_classes=len(self.df.labels.unique()))
        return X, labels, X_pos1, X_pos2

    def get_entity1_position(self, sentence):
        for i, v in enumerate(sentence.split()):
            if "<e1>" in v:
                return i

    def get_entity2_position(self, sentence):
        for i, v in enumerate(sentence.split()):
            if "<e2>" in v:
                return i

prepocessed_data = HandleDataset("/content/gdrive/MyDrive/CS6320/dataset/SemEval2010_task8_training/TRAIN_FILE.TXT")

#print(prepocessed_data.df[:5])

prepocessed_data.df

prepocessed_data.df["Positon of e1"]

"""### Get training dataset and testing dataset"""

X, labels, X_pos1, X_pos2 = prepocessed_data.get_tokens()

X[:5]

labels[:5]

labels.shape

X_pos1.shape

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test, X_train_pos1, X_test_pos1, X_train_pos2, X_test_pos2 = train_test_split(X , labels, X_pos1, X_pos2, test_size=0.20, random_state=42)

X_train.shape

X_train_pos1.shape

len(X_train_pos2)

y_train.shape

"""## Training the model"""

# set up the model
from keras.models import Sequential, Model
from keras.layers import InputLayer, Activation
from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Input
n_most_common_words = 19605

embedding_size = 300
# model = Sequential()
# model.add(Embedding(n_most_common_words, embedding_size, input_length=X.shape[1]))
# model.add(Bidirectional(LSTM(128, dropout=0.7, recurrent_dropout=0.7)))
# model.add(Dense(labels.shape[1], activation='softmax'))
# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])
# print(model.summary())

embed_layer = Embedding(n_most_common_words, embedding_size)
input_seq = Input(shape=(X_train.shape[1],))
embed_seq = embed_layer(input_seq)

embed_seq

input_seq_pos1 = Input(shape=(X_train_pos1.shape[1],))
embed_seq_pos1 = Embedding(100,100,input_length=max_len,trainable=True)(input_seq_pos1)
input_seq_pos2 = Input(shape=(X_train_pos2.shape[1],))
embed_seq_pos2 = Embedding(100,100,input_length=max_len,trainable=True)(input_seq_pos2)

embed_seq_pos2

x = tf.keras.layers.concatenate([embed_seq, embed_seq_pos1, embed_seq_pos2])
x = Bidirectional(LSTM(128, dropout=0.5, recurrent_dropout=0.7))(x)
# x = Bidirectional(LSTM(256, dropout=0.7, recurrent_dropout=0.7))(x)
#x = Dense(256, activation='relu')(x)
preds = Dense(labels.shape[1],activation="softmax")(x)
model = Model(inputs=[input_seq, input_seq_pos1, input_seq_pos2], outputs=preds)
model.compile(loss="categorical_crossentropy",optimizer="adam",metrics=["accuracy"])
print(model.summary())

#tf.config.experimental_run_functions_eagerly(True)

# model.fit(X_train, y_train, epochs=3, batch_size=128, validation_split=0.2)
model.fit([X_train, X_train_pos1, X_train_pos2], y_train, epochs=4,batch_size=128, validation_split=0.2)

from sklearn.metrics import confusion_matrix, classification_report
import numpy as np
# prediction_probas = model.predict(X_test)
prediction_probas = model.predict([X_test, X_test_pos1, X_test_pos2])

prediction_probas[:5]

predictions = [np.argmax(pred) for pred in prediction_probas]

predictions

y_test_labels = [np.argmax(prob) for prob in y_test]

y_test_labels[:5]

for i in y_test_labels:
    if i == 6:
        print("we have 6")

print(confusion_matrix(y_test_labels, predictions)) #from sklearn.metrics

print(classification_report(y_test_labels, predictions, digits=3))